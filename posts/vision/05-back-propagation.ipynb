{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Backpropagation | Log #005\"\n",
    "description: How does the NN learn ?\"\n",
    "date: 2025-01-05\n",
    "author: \"Hemanth KR\"\n",
    "categories: [Vision]\n",
    "image: \"thumbnails/005.jpg\"\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Previously we saw how to build a neural network that does image classification. Before we explore different neural network architectures, let's spend some time understanding how the NN actually learns.  \n",
    "\n",
    "[Source Link to checkout.](https://www.dataversity.net/brief-history-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By 2011, the speed of GPUs had increased significantly, making it possible to train convolutional neural networks “without” the layer-by-layer pre-training. With the increased computing speed, it became obvious deep learning had significant advantages in terms of efficiency and speed. One example is AlexNet, a convolutional neural network whose architecture won several international competitions during 2011 and 2012. Rectified linear units were used to enhance the speed and dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Forward pass\n",
    "* loss functions\n",
    "* calculate losses and gradients with backward pass\n",
    "* Update gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Backpropagation\n",
    "The backpropagation algorithm consists of several key steps:\n",
    "Forward Pass:\n",
    "Input data is fed into the network.\n",
    "The data propagates through the layers, with each neuron applying weights, biases, and activation functions.\n",
    "The network produces an output.\n",
    "Error Calculation:\n",
    "The difference between the network's output and the desired output is calculated.\n",
    "Backward Pass:\n",
    "The error is propagated backwards through the network.\n",
    "Gradients of the error with respect to each weight are computed using the chain rule.\n",
    "Weight Update:\n",
    "Weights are adjusted to minimize the error, typically using an optimization algorithm like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
