{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Activation to Optimization - Part 2 | Log #007\"\n",
    "description: \"Understanding the building blocks of neural networks\"\n",
    "date: 2025-01-28\n",
    "author: \"Hemanth KR\"\n",
    "categories: [Vision]\n",
    "image: \"thumbnails/007.jpg\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Loss functions in deep learning are used to quantify how the model is performing. It calculates the error between model's predicted values and the actual target values. The aim is to improve the model's performance by adjusting the parameters in the direction that minimizes the loss and improves predictive accuracy. Broadly there are two main types of loss functions - Regression Loss functions and Classification Loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Loss Functions\n",
    "\n",
    "1. Mean Absolute Error (MAE) - Measures the absolute difference between the predicted and actual values. Also known as L1 loss. Robust to outliers. Not differentiable at zero causing difficulties during optimisations.\n",
    "\n",
    "    $$MAE = \\frac{\\sum_{i=1}^{n} |y_i - \\hat{y_i}|}{n}$$\n",
    "\n",
    "2. Mean Squared Error (MSE) - Measures the average squared difference between predicted and actual values. Also known as L2 loss. More sensitive to outliers. Differentiable everywhere making it easier for optimisation algorithms like gradient descent.\n",
    "\n",
    "    $$MSE = \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n}$$\n",
    "\n",
    "3. Huber Loss - Huber loss combines the benifits of MSE and MAE. Less sensitive to outliers than MSE and differentiable everywhere unlike MAE. Extremely useful when the data contains noise.\n",
    "\n",
    "    $$L_\\delta(y, \\hat{y}) = \\begin{cases}\n",
    "                                \\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "                                \\delta|y - \\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise}\n",
    "                             \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Loss Functions\n",
    "\n",
    "1. Binary Cross-Entropy Loss - Measures the performance of a classification model whose output is a probability value between 0 and 1. Also known as Log Loss used for binary classification problems.\n",
    "\n",
    "    $$BCE = -\\frac{1}{N}\\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$\n",
    "\n",
    "2. Categorical Cross-Entropy Loss - Used for multi-class classification problems. Measures the performance of a classification model whose output is a probability disctribution over multiple classes.\n",
    "\n",
    "    $$CCE = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{C} y_{ij}\\log(\\hat{y}_{ij})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Loss functions play a crucial role in training neural networks by providing a quantitative measure of how well the model is performing. They act as a compass, guiding the optimization process by indicating how far the model's predictions deviate from the actual values. The choice of loss function depends on the specific problem type - regression loss functions like MAE, MSE, and Huber Loss are used for continuous value predictions, while classification loss functions like Binary and Categorical Cross-Entropy are used for discrete class predictions.\n",
    "\n",
    "Understanding these loss functions is essential as they directly impact model training and performance. Each loss function has its own characteristics - some are more robust to outliers, some provide better gradients for optimization, and others are specifically designed for certain types of problems. By choosing the appropriate loss function and understanding its properties, we can effectively train neural networks to achieve optimal performance on our specific tasks. The next blog will complete this series by looking at different types of activation functions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
