{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Activation to Optimization - Part 3 | Log #008\"\n",
    "description: \"Understanding the building blocks of neural networks\"\n",
    "date: 2025-01-28\n",
    "author: \"Hemanth KR\"\n",
    "categories: [Vision]\n",
    "image: \"thumbnails/008.jpg\"\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithms\n",
    "\n",
    "Gradient descent is the technique used to optimise the neural network. It is designed to mimimise the cost or loss function of the model by iteratively adjusting it's parameters to reach the lowest possible error between predicted and actual outcomes. The gradient of a function at a point is a vector that points in the direction of the steepest ascent. It is calculated using partial derivatives if the function is multivariable. During optimisation, the goal is often to minimise a function. Moving in the direction of the steepest descent also called as negative gradient, ensures the function value decreases at the fastest rate possible from the current point.\n",
    "In general the gradient descent update rule for parameters $\\theta$ is given by:\n",
    "$$\\theta_{new} = \\theta_{old} - \\alpha \\cdot \\nabla f(\\theta_{old}) $$\n",
    "where:\n",
    "\n",
    "* $\\alpha$ is the learning rate\n",
    "* $\\nabla f(\\theta_{old})$ is the gradient of the cost function at $\\theta_{old}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
